---
title: "Neural Networks"
author: "Shantam Gupta"
date: "May 13, 2018"
output:
   pdf_document:
    highlight: tango
    latex_engine: lualatex
    number_sections: yes
    toc: yes
---

# Installing the Package
```{r, echo=FALSE, warning=FALSE, message=FALSE}
#install.packages("neuralnet")
library(neuralnet)
library(nnet)
library(dplyr)
library(caret)
install.packages("pdp")
library(pdp)
library(grid)
```

# Load the Data 
```{r, echo = F}
Train <- read.csv('lumos_training_set.csv')
Test <- read.csv('lumos_all_set.csv')

#remove repeated measurements and reshape the dataset
ind <- which(with( Train, (Train$PepSeq=="EYEATLEEC(Carbamidomethyl)C(Carbamidomethyl)AK" | Train$PepSeq=="TC(Carbamidomethyl)VADESHAGC(Carbamidomethyl)EK") ))
S0<-Train[-ind,]
S0<-S0[,-2]
Train<-S0
S0$PepSeq<- gsub("\\(Carbamidomethyl\\)","",S0$PepSeq)
S0 <- reshape(S0, idvar = "idfile", timevar = "PepSeq", direction = "wide")
RESPONSE<-c("GO")
S0 <- cbind(S0,RESPONSE)

ind <- which(with( Test, (Test$PepSeq=="EYEATLEEC(Carbamidomethyl)C(Carbamidomethyl)AK" | Test$PepSeq=="TC(Carbamidomethyl)VADESHAGC(Carbamidomethyl)EK") ))
Data0<-Test[-ind,]
Data0<-Data0[,-2]
Data0$PepSeq<- gsub("\\(Carbamidomethyl\\)","",Data0$PepSeq)
Data1 <- Data0[1:8 + rep(seq(0, nrow(Data0), by=100), each=8),]
Data1 <- reshape(Data1, idvar = "idfile", timevar = "PepSeq", direction = "wide")
RESPONSE<-c("NOGO")
Data <- cbind(Data1,RESPONSE)
```

# Preprocess the data: Normalize the data
```{r}
new_data <- rbind(S0,Data)
maxs <- apply(new_data %>% select(-c(idfile,RESPONSE)), 2, max) 
mins <- apply(new_data %>% select(-c(idfile,RESPONSE)), 2, min)

scaled_data <- as.data.frame(scale(new_data %>% select(-c(idfile,RESPONSE)), center = mins, scale = maxs - mins))
#scaled_data$RESPONSE <- ifelse(new_data$RESPONSE =="GO",1,0)
scaled_data$RESPONSE <- as.factor(new_data$RESPONSE)
#scaled_data$RESPONSE <- as.factor(scaled_data$RESPONSE)
scaled_data$idfile <- new_data$idfile

#select random ind for train and test 
set.seed(123)

## 75% of the sample size
smp_size <- floor(0.75 * nrow(scaled_data))

## set the seed to make your partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(scaled_data)), size = smp_size)


train <- scaled_data[train_ind,]
test <- scaled_data[-train_ind,]
```

#Building Neural Network

```{r results='hide'}
library(h2o)
#generate same set of random numbers (for reproducibility)
set.seed(121)

#launch h2o cluster
localH2O <- h2o.init(nthreads = -1)


#import r objects to h2o cloud
train_h2o <- as.h2o(train)
test_h2o <- as.h2o(test)
```

```{r echo= FALSE}
#disable progress bar for pdf output
h2o.no_progress()
```

```{r}
#build the mlp(multi layer perceptron) deep learning model using h2o
set.seed(100)

dl_model <- h2o.deeplearning(
  model_id="dl_model_first", 
  training_frame=train_h2o, 
  validation_frame = test_h2o,
  x= colnames(train_h2o[,1:48]),
  y= "RESPONSE",
  activation="Rectifier",  
  hidden=c(5,4), 
  stopping_metric="mean_per_class_error",
  stopping_tolerance=0.01,
  epochs=100        
)
```

```{r}
summary(dl_model)
```
The accuracy is 98.57% . The net could be optmizied further to improve the accuracy 

### Tuning the ANN
The simplest hyperparameter search method is a brute-force scan of the full Cartesian product of all combinations specified by a grid search. There are a lot of paramters to tune and due to limited computational capabilities we shall try to tune only some of them.
```{r,results='hide', eval=FALSE}
#hyperparamters to tune 
hyper_params <- list(
  hidden=list(c(32,32,32),c(50,200,50)),  # different architectures of hidden layer
  input_dropout_ratio=c(0,0.05),      # values for drop out
  rate=c(0.01,0.02),                  # the learning rae
  activation = c("Rectifier")   # activation functions
)

#grid search

grid <- h2o.grid(
  algorithm="deeplearning",
  grid_id="dl_grid",
  model_id="dl_model_first", 
  training_frame=train_h2o, 
  x= colnames(train_h2o[,1:12]),
  y= "label",
  stopping_metric="mean_per_class_error",
  hyper_params = hyper_params,
  epochs=1000,   
  stopping_tolerance=0.01,
  variable_importances=T    
)
```

```{r tidy= TRUE, eval=FALSE}
#sort the model in the grid in decreasing order of error 
grid <- h2o.getGrid("dl_grid",sort_by="err",decreasing=FALSE)
grid

#best model and its full set of parameters
grid@summary_table[1,]
best_dl_model <- h2o.getModel(grid@model_ids[[1]])
best_dl_model

print(h2o.performance(best_dl_model))

#storing the confusion matrix
best_dl_confusion <- as.data.frame(h2o.confusionMatrix(best_dl_model))

```

### Plotting  the model
```{r}
plot(dl_model,timesteps = "epochs",metric = "classification_error")
```

The training accuracy decreases with increase in epochs. However, this might lead to overfitting on training data and poor fit on the test data. 

### Predictons on test data
```{r warning=FALSE, error=FALSE, message=F}
dl_predict <- as.data.frame(h2o.predict(dl_model, test_h2o))
```

### Variable importance
```{r}
h2o.varimp_plot(dl_model)
```


### Plotting decision boundary for neural networks 

```

```{r}
PlotGrid <- function(data, PredictorA, PredictorB, pred,title) {
  
  nbp <- 353;
  PredA <- seq(min(PredictorA), max(PredictorA), length = nbp)
  PredB <- seq(min(PredictorB), max(PredictorB), length = nbp)
  Grid <- expand.grid(PredictorA = PredA, PredictorB = PredB)
  surf <- (ggplot(data = data, aes(x = PredictorA, y = PredictorB, 
                                      color = as.factor(data$RESPONSE))) +
          geom_tile(data = cbind(Grid, classes = pred), aes(fill =data$RESPONSE))) +
          scale_fill_manual(name = 'classes', values = c("RED","GREEN")) +
          ggtitle("Decision region") + theme(legend.text = element_text(size = 10)) +
  scale_colour_manual(name = 'classes', values = c("RED","GREEN"))) +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0))
  pts <- (ggplot(data = data, aes(x = PredictorA, y = PredictorB,  
                                    color = as.factor(data$RESPONSE))) +
          geom_contour(data = cbind(Grid, classes = pred), aes(z = as.factor(data$RESPONSE)), 
                       color = c("RED","GREEN"), breaks = c(1.5)) +
          geom_point(size = 4, alpha = .5) + 
          ggtitle("Decision boundary") +
          theme(legend.text = element_text(size = 10)) +
          scale_colour_manual(name = 'classes', values = c("RED","GREEN"))) +
    scale_x_continuous(expand = c(0,0)) +
    scale_y_continuous(expand = c(0,0))
  grid.arrange(surf, pts, top = textGrob(title, gp = gpar(fontsize = 20)), ncol = 2)
}
```

Ref : http://www.cmap.polytechnique.fr/~lepennec/R/Learning/Learning.html


```{r}
pred = test_pred$predict
nbp <- 353;
PredA <- seq(min(test$TotalArea.LVNELTEFAK), max(test$TotalArea.LVNELTEFAK), length.out =  nbp)
PredB <- seq(min(test$MassAccu.HLVDEPQNLIK), max(test$MassAccu.HLVDEPQNLIK), length.out = nbp)
Grid <- expand.grid(PredictorA = PredA, PredictorB = PredB)
ggplot(data = test, aes(x = PredictorA, y = PredictorB, 
                                      color = data$RESPONSE)) +
          geom_tile(data = cbind(Grid, classes = pred), aes(fill = data$RESPONSE)) +
          scale_fill_manual(name = 'classes', values = c("RED","GREEN")) +
          ggtitle("Decision region") + theme(legend.text = element_text(size = 10)) +
  scale_colour_manual(name = 'classes', values = c("RED","GREEN")) +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0))
```


```{r}
test_pred <- h2o.predict(dl_model, newdata = test_h2o[1:48])
test_pred <- as.data.frame(test_pred)



plot(train[,c("TotalArea.LVNELTEFAK","MassAccu.HLVDEPQNLIK")], pch=19, col=train$color, 
cex=3, xlim=c(-4,4), ylim=c(-4,4))
train$color = ifelse(train$RESPONSE == "GO","green","red")
plot(train[,c("MassAccu.SLHTLFGDELCK","MassAccu.HLVDEPQNLIK")], pch=19, col=train$color, 
cex=3, xlim=c(-4,4), ylim=c(-4,4))


Pred <- factor((test_pred$predict), 
               levels = c('Class1','Class2'))
xx <- PlotGrid(test,test$TotalArea.LVNELTEFAK, test$MassAccu.HLVDEPQNLIK, Pred, "NN with H2o")
````

```{r echo = FALSE, results='hide'}
#shut down the cluster
h2o.shutdown() 
```

### Partial dependence plots and maps
```{r}
pdpTA <- partial(dl_model, pred.var = "TotalArea.LVNELTEFAK",plot = TRUE, prob=TRUE, rug = TRUE,train = train_h2o)
pdpRT <- partial(fit, pred.var = "RT.LVNELTEFAK", plot = TRUE, prob=TRUE, rug=TRUE)
pdpMA <- partial(fit, pred.var = "MassAccu.LVNELTEFAK", plot = TRUE, prob=TRUE, rug = TRUE)
pdpFWHM <- partial(fit, pred.var = "FWHM.LVNELTEFAK", plot = TRUE, prob=TRUE, rug = TRUE)
grid.arrange(pdpRT, pdpTA, ncol = 2)
grid.arrange(pdpMA, pdpFWHM, ncol = 2)
````


