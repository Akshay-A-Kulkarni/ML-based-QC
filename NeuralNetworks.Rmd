---
title: "Neural Networks"
author: "Shantam Gupta"
date: "May 13, 2018"
output:
   pdf_document:
    highlight: tango
    latex_engine: lualatex
    number_sections: yes
    toc: yes
---

# Installing the Package
```{r, echo=FALSE, warning=FALSE, message=FALSE}
#install.packages("neuralnet")
library(neuralnet)
library(nnet)
library(dplyr)
library(caret)
#install.packages("pdp")
library(pdp)
library(grid)
```

# Load the Data 
```{r, echo = F}
Train <- read.csv('lumos_training_set.csv')
Test <- read.csv('lumos_all_set.csv')

#remove repeated measurements and reshape the dataset
ind <- which(with( Train, (Train$PepSeq=="EYEATLEEC(Carbamidomethyl)C(Carbamidomethyl)AK" | Train$PepSeq=="TC(Carbamidomethyl)VADESHAGC(Carbamidomethyl)EK") ))
S0<-Train[-ind,]
S0<-S0[,-2]
Train<-S0
S0$PepSeq<- gsub("\\(Carbamidomethyl\\)","",S0$PepSeq)
S0 <- reshape(S0, idvar = "idfile", timevar = "PepSeq", direction = "wide")
RESPONSE<-c("GO")
S0 <- cbind(S0,RESPONSE)

ind <- which(with( Test, (Test$PepSeq=="EYEATLEEC(Carbamidomethyl)C(Carbamidomethyl)AK" | Test$PepSeq=="TC(Carbamidomethyl)VADESHAGC(Carbamidomethyl)EK") ))
Data0<-Test[-ind,]
Data0<-Data0[,-2]
Data0$PepSeq<- gsub("\\(Carbamidomethyl\\)","",Data0$PepSeq)
Data1 <- Data0[1:8 + rep(seq(0, nrow(Data0), by=100), each=8),]
Data1 <- reshape(Data1, idvar = "idfile", timevar = "PepSeq", direction = "wide")# all no gos
RESPONSE<-c("NOGO")
Data <- cbind(Data1,RESPONSE)
```

# Preprocess the data: Normalize the data
```{r}
new_data <- rbind(S0,Data)
maxs <- apply(new_data %>% select(-c(idfile,RESPONSE)), 2, max) 
mins <- apply(new_data %>% select(-c(idfile,RESPONSE)), 2, min)

scaled_data <- as.data.frame(scale(new_data %>% select(-c(idfile,RESPONSE)), center = mins, scale = maxs - mins))
#scaled_data$RESPONSE <- ifelse(new_data$RESPONSE =="GO",1,0)
scaled_data$RESPONSE <- as.factor(new_data$RESPONSE)
#scaled_data$RESPONSE <- as.factor(scaled_data$RESPONSE)
scaled_data$idfile <- new_data$idfile

#select random ind for train and test 
set.seed(123)

## 75% of the sample size
smp_size <- floor(0.75 * nrow(scaled_data))

## set the seed to make your partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(scaled_data)), size = smp_size)


train <- scaled_data[train_ind,]
test <- scaled_data[-train_ind,]
```

#Building Neural Network

```{r results='hide'}
library(h2o)
#generate same set of random numbers (for reproducibility)
set.seed(121)

#launch h2o cluster
localH2O <- h2o.init(nthreads = -1)


#import r objects to h2o cloud
train_h2o <- as.h2o(train)
test_h2o <- as.h2o(test)
```

```{r echo= FALSE}
#disable progress bar for pdf output
h2o.no_progress()
```

```{r}
#build the mlp(multi layer perceptron) deep learning model using h2o
set.seed(100)

dl_model <- h2o.deeplearning(
  model_id="dl_model_first", 
  training_frame=train_h2o, 
  validation_frame = test_h2o,
  x= colnames(train_h2o[,1:48]),
  y= "RESPONSE",
  activation="tanh",  
  hidden=c(5,4), 
  stopping_metric="mean_per_class_error",
  stopping_tolerance=0.01,
  epochs=100,
  seed = 123, # give seed 
  export_weights_and_biases = T, # export weights and biases defaults to false
  reproducible = T # Force reproducibility on small data (will be slow - only uses 1 thread). Defaults to FALSE.
)
```

```{r}
summary(dl_model)
```
The accuracy is 98.57% . The net could be optmizied further to improve the accuracy 




#build the mlp(multi layer perceptron) deep learning model2 using h2o
```{r}
set.seed(100)

dl_model2 <- h2o.deeplearning(
  model_id="dl_model_first", 
  training_frame=train_h2o, 
  validation_frame = test_h2o,
  x= colnames(train_h2o[,1:48]),
  y= "RESPONSE",
  activation="tanh",  
  hidden=c(5,4), 
  stopping_metric="mean_per_class_error",
  stopping_tolerance=0.01,
  epochs=100,
  seed = 123, # give seed 
  export_weights_and_biases = T, # export weights and biases defaults to false
  reproducible = T # Force reproducibility on small data (will be slow - only uses 1 thread). Defaults to FALSE.
)
```






### Tuning the ANN
The simplest hyperparameter search method is a brute-force scan of the full Cartesian product of all combinations specified by a grid search. There are a lot of paramters to tune and due to limited computational capabilities we shall try to tune only some of them.
```{r,results='hide', eval=FALSE}
#hyperparamters to tune 
hyper_params <- list(
  hidden=list(c(32,32,32),c(50,200,50)),  # different architectures of hidden layer
  input_dropout_ratio=c(0,0.05),      # values for drop out
  rate=c(0.01,0.02),                  # the learning rae
  activation = c("Rectifier")   # activation functions
)

#grid search

grid <- h2o.grid(
  algorithm="deeplearning",
  grid_id="dl_grid",
  model_id="dl_model_first", 
  training_frame=train_h2o, 
  x= colnames(train_h2o[,1:12]),
  y= "label",
  stopping_metric="mean_per_class_error",
  hyper_params = hyper_params,
  epochs=1000,   
  stopping_tolerance=0.01,
  variable_importances=T
)

```

```{r tidy= TRUE, eval=FALSE}
#sort the model in the grid in decreasing order of error 
grid <- h2o.getGrid("dl_grid",sort_by="err",decreasing=FALSE)
grid

#best model and its full set of parameters
grid@summary_table[1,]
best_dl_model <- h2o.getModel(grid@model_ids[[1]])
best_dl_model

print(h2o.performance(best_dl_model))

#storing the confusion matrix
best_dl_confusion <- as.data.frame(h2o.confusionMatrix(best_dl_model))

```

### Plotting  the model
```{r}
plot(dl_model,timesteps = "epochs",metric = "classification_error")
```

The training accuracy  and testing accuracy decreases with increase in epochs. 

### Predictons on test data
```{r warning=FALSE, error=FALSE, message=F}
dl_predict <- as.data.frame(h2o.predict(dl_model, test_h2o))
```

### Variable importance
```{r}
h2o.varimp_plot(dl_model)
```

### Getting weights for neural network
```{r}
weights1<- h2o.weights(dl_model,matrix_id = 1)
print(head(weights1))
```

### Plotting decision boundary for neural networks 

```{r}
Color <- c("GREEN","RED")
names(Color) <- c('GO','NOGO')
ggplot(data = test,aes(x = test$TotalArea.LVNELTEFAK, y = test$MassAccu.HLVDEPQNLIK)) + 
  geom_point(aes(color = test$RESPONSE), size = 6, alpha = .5) +
  scale_colour_manual(name = 'classes', values = Color) +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0))
```



#build the mlp(multi layer perceptron) deep learning model using h2o
```{r}
set.seed(100)

dl_model2 <- h2o.deeplearning(
  model_id="dl_model_first", 
  training_frame=train_h2o, 
  validation_frame = test_h2o,
  x= colnames(train_h2o[,1:48]),
  y= "RESPONSE",
  activation="Tanh",  
  hidden=c(5,5), 
  stopping_metric="mean_per_class_error",
  stopping_rounds = 5,
  stopping_tolerance=0.01,
  rate = 0.005, # Defaults to 0.005
  mini_batch_size = 1,# defaults to 1 
  epochs=100,
  seed = 123, # give seed 
  export_weights_and_biases = T, # export weights and biases defaults to false
  reproducible = T # Force reproducibility on small data (will be slow - only uses 1 thread). Defaults to FALSE.
)
summary(dl_model2)
plot(dl_model2)

```



###  shutdown h2o
```{r echo = FALSE, results='hide'}
#shut down the cluster
h2o.shutdown() 
```


## Simulating 3 effects 

## Main Data 
```{r}
head(Train)
nrow(Train)
Train %>% group_by(PepSeq) %>% summarise(Count = n())
```


### Simulation 1: Spike in mean X 10 
```{r}
spike_mean <- function(num_col,value){
  # Train Data 
  #one peptide LVNELTEFAK 
  
  #generate multivariate normal data
  #parameters from a training sample
  n<-100 #incontrol observations 
  m<-100 #ooc observations
  
  # Simularing in Control data with n observations
  mean <-c(with(data=Train,tapply(RT,INDEX=PepSeq,FUN=mean))[5],
           with(data=Train,tapply(TotalArea,INDEX=PepSeq,FUN=mean)) [5],
           with(data=Train,tapply(MassAccu,INDEX=PepSeq,FUN=mean)) [5],
           with(data=Train,tapply(FWHM,INDEX=PepSeq,FUN=mean)) [5]
           )
  covar<- cov(Train[Train$PepSeq=="LVNELTEFAK",c(3,6,7,8)])
  
  
  Sim_ic_1 <-data.frame(idfile=1:n,PepSeq=rep("LVNELTEFAK",n),mvrnorm(n, mean, covar))
  colnames(Sim_ic_1)<-c("idfile","PepSeq","RT","TotalArea","MassAccu","FWHM")
  
  RESPONSE <- c("GO")
  Sim_ic_1 <- cbind(Sim_ic_1,RESPONSE) # simulation effect 1  incontrol observation 
  
  
  
  # Simulating Out of Control data with m observations
  # Sim_oc_1 <-data.frame(idfile=(n+1):(n+m),PepSeq=rep("LVNELTEFAK",m),
  #                   mvrnorm(m, mean+(10*c(covar[1,1],1.0*covar[2,2],3.0*covar[3,3],1.0*covar[4,4])), 
  #                  covar))
  if(num_col == 1)
    Sim_oc_1 <- data.frame(idfile=(n+1):(n+m),PepSeq=rep("LVNELTEFAK",m), # increase in mean FWHM X 10 
                    mvrnorm(m, mean*c(value,1,1,1), 
                   covar))
  else if(num_col == 2)
     Sim_oc_1 <- data.frame(idfile=(n+1):(n+m),PepSeq=rep("LVNELTEFAK",m), # increase in mean FWHM X 10 
                    mvrnorm(m, mean*c(1,value,1,1), 
                   covar))
  else if(num_col == 3)
     Sim_oc_1 <- data.frame(idfile=(n+1):(n+m),PepSeq=rep("LVNELTEFAK",m), # increase in mean FWHM X 10 
                    mvrnorm(m, mean*c(1,1,value,1), 
                   covar))
  else
     Sim_oc_1 <- data.frame(idfile=(n+1):(n+m),PepSeq=rep("LVNELTEFAK",m), # increase in mean FWHM X 10 
                    mvrnorm(m, mean*c(1,1,1,value), 
                   covar))
  
  colnames(Sim_oc_1) <- c("idfile","PepSeq","RT","TotalArea","MassAccu","FWHM")
  
  RESPONSE <- c("NOGO")
  Sim_oc_1 <- cbind(Sim_oc_1,RESPONSE)
  new_data <- rbind(Sim_ic_1,Sim_oc_1)
return(new_data)
}
```

```{r}
new_data <- spike_mean(1,10)
maxs <- apply(new_data %>% dplyr::select(-c(idfile,PepSeq, RESPONSE)), 2, max) 
mins <- apply(new_data %>% dplyr::select(-c(idfile,PepSeq, RESPONSE)), 2, min)

scaled_data <- as.data.frame(scale(new_data %>% dplyr::select(-c(idfile,PepSeq,RESPONSE)), center = mins, scale = maxs - mins))
#scaled_data$RESPONSE <- ifelse(new_data$RESPONSE =="GO",1,0)
scaled_data$RESPONSE <- as.factor(new_data$RESPONSE)
#scaled_data$RESPONSE <- as.factor(scaled_data$RESPONSE)
scaled_data$idfile <- new_data$idfile

#select random ind for train and test 
set.seed(123)

## 75% of the sample size
smp_size <- floor(0.75 * nrow(scaled_data))

## set the seed to make your partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(scaled_data)), size = smp_size)


train_sim1 <- scaled_data[train_ind,]
test_sim1 <- scaled_data[-train_ind,]

#Building Neural Network
library(h2o)
#generate same set of random numbers (for reproducibility)
set.seed(121)

#launch h2o cluster
localH2O <- h2o.init(nthreads = -1)


#import r objects to h2o cloud
train_h2o <- as.h2o(train_sim1)
test_h2o <- as.h2o(test_sim1)

set.seed(100)

dl_model_sim1 <- h2o.deeplearning(
  model_id="dl_model_first", 
  training_frame=train_h2o, 
  validation_frame = test_h2o,
  x= colnames(train_h2o[,1:4]),
  y= "RESPONSE",
  activatio="Tanh",  
  hidden=c(2,2), 
  stopping_metric="mean_per_class_error",
  stopping_rounds = 5,
  stopping_tolerance=0.001,
  rate = 0.005, # Defaults to 0.005
  mini_batch_size = 1,# defaults to 1 
  epochs=100,
  seed = 123, # give seed 
  export_weights_and_biases = T, # export weights and biases defaults to false
  reproducible = T # Force reproducibility on small data (will be slow - only uses 1 thread). Defaults to FALSE.
)
summary(dl_model_sim1)
plot(dl_model_sim1)
```

```{r}
new_data <- spike_mean(2,10)
maxs <- apply(new_data %>% dplyr::select(-c(idfile,PepSeq, RESPONSE)), 2, max) 
mins <- apply(new_data %>% dplyr::select(-c(idfile,PepSeq, RESPONSE)), 2, min)

scaled_data <- as.data.frame(scale(new_data %>% dplyr::select(-c(idfile,PepSeq,RESPONSE)), center = mins, scale = maxs - mins))
#scaled_data$RESPONSE <- ifelse(new_data$RESPONSE =="GO",1,0)
scaled_data$RESPONSE <- as.factor(new_data$RESPONSE)
#scaled_data$RESPONSE <- as.factor(scaled_data$RESPONSE)
scaled_data$idfile <- new_data$idfile

#select random ind for train and test 
set.seed(123)

## 75% of the sample size
smp_size <- floor(0.75 * nrow(scaled_data))

## set the seed to make your partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(scaled_data)), size = smp_size)


train_sim1 <- scaled_data[train_ind,]
test_sim1 <- scaled_data[-train_ind,]

#Building Neural Network
library(h2o)
#generate same set of random numbers (for reproducibility)
set.seed(121)

#launch h2o cluster
localH2O <- h2o.init(nthreads = -1)


#import r objects to h2o cloud
train_h2o <- as.h2o(train_sim1)
test_h2o <- as.h2o(test_sim1)

set.seed(100)

dl_model_sim1 <- h2o.deeplearning(
  model_id="dl_model_first", 
  training_frame=train_h2o, 
  validation_frame = test_h2o,
  x= colnames(train_h2o[,1:4]),
  y= "RESPONSE",
  activatio="Tanh",  
  hidden=c(2,2), 
  stopping_metric="mean_per_class_error",
  stopping_rounds = 5,
  stopping_tolerance=0.001,
  rate = 0.005, # Defaults to 0.005
  mini_batch_size = 1,# defaults to 1 
  epochs=100,
  seed = 123, # give seed 
  export_weights_and_biases = T, # export weights and biases defaults to false
  reproducible = T # Force reproducibility on small data (will be slow - only uses 1 thread). Defaults to FALSE.
)
summary(dl_model_sim1)
plot(dl_model_sim1)
```



```{r}
new_data <- spike_mean(3,10)
maxs <- apply(new_data %>% dplyr::select(-c(idfile,PepSeq, RESPONSE)), 2, max) 
mins <- apply(new_data %>% dplyr::select(-c(idfile,PepSeq, RESPONSE)), 2, min)

scaled_data <- as.data.frame(scale(new_data %>% dplyr::select(-c(idfile,PepSeq,RESPONSE)), center = mins, scale = maxs - mins))
#scaled_data$RESPONSE <- ifelse(new_data$RESPONSE =="GO",1,0)
scaled_data$RESPONSE <- as.factor(new_data$RESPONSE)
#scaled_data$RESPONSE <- as.factor(scaled_data$RESPONSE)
scaled_data$idfile <- new_data$idfile

#select random ind for train and test 
set.seed(123)

## 75% of the sample size
smp_size <- floor(0.75 * nrow(scaled_data))

## set the seed to make your partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(scaled_data)), size = smp_size)


train_sim1 <- scaled_data[train_ind,]
test_sim1 <- scaled_data[-train_ind,]

#Building Neural Network
library(h2o)
#generate same set of random numbers (for reproducibility)
set.seed(121)

#launch h2o cluster
localH2O <- h2o.init(nthreads = -1)


#import r objects to h2o cloud
train_h2o <- as.h2o(train_sim1)
test_h2o <- as.h2o(test_sim1)

set.seed(100)

dl_model_sim1 <- h2o.deeplearning(
  model_id="dl_model_first", 
  training_frame=train_h2o, 
  validation_frame = test_h2o,
  x= colnames(train_h2o[,1:4]),
  y= "RESPONSE",
  activatio="Tanh",  
  hidden=c(2,2), 
  stopping_metric="mean_per_class_error",
  stopping_rounds = 5,
  stopping_tolerance=0.001,
  rate = 0.005, # Defaults to 0.005
  mini_batch_size = 1,# defaults to 1 
  epochs=100,
  seed = 123, # give seed 
  export_weights_and_biases = T, # export weights and biases defaults to false
  reproducible = T # Force reproducibility on small data (will be slow - only uses 1 thread). Defaults to FALSE.
)
summary(dl_model_sim1)
plot(dl_model_sim1)
```


```{r}
new_data <- spike_mean(4,10)
maxs <- apply(new_data %>% dplyr::select(-c(idfile,PepSeq, RESPONSE)), 2, max) 
mins <- apply(new_data %>% dplyr::select(-c(idfile,PepSeq, RESPONSE)), 2, min)

scaled_data <- as.data.frame(scale(new_data %>% dplyr::select(-c(idfile,PepSeq,RESPONSE)), center = mins, scale = maxs - mins))
#scaled_data$RESPONSE <- ifelse(new_data$RESPONSE =="GO",1,0)
scaled_data$RESPONSE <- as.factor(new_data$RESPONSE)
#scaled_data$RESPONSE <- as.factor(scaled_data$RESPONSE)
scaled_data$idfile <- new_data$idfile

#select random ind for train and test 
set.seed(123)

## 75% of the sample size
smp_size <- floor(0.75 * nrow(scaled_data))

## set the seed to make your partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(scaled_data)), size = smp_size)


train_sim1 <- scaled_data[train_ind,]
test_sim1 <- scaled_data[-train_ind,]

#Building Neural Network
library(h2o)
#generate same set of random numbers (for reproducibility)
set.seed(121)

#launch h2o cluster
localH2O <- h2o.init(nthreads = -1)


#import r objects to h2o cloud
train_h2o <- as.h2o(train_sim1)
test_h2o <- as.h2o(test_sim1)

set.seed(100)

dl_model_sim1 <- h2o.deeplearning(
  model_id="dl_model_first", 
  training_frame=train_h2o, 
  validation_frame = test_h2o,
  x= colnames(train_h2o[,1:4]),
  y= "RESPONSE",
  activatio="Tanh",  
  hidden=c(2,2), 
  stopping_metric="mean_per_class_error",
  stopping_rounds = 5,
  stopping_tolerance=0.001,
  rate = 0.005, # Defaults to 0.005
  mini_batch_size = 1,# defaults to 1 
  epochs=100,
  seed = 123, # give seed 
  export_weights_and_biases = T, # export weights and biases defaults to false
  reproducible = T # Force reproducibility on small data (will be slow - only uses 1 thread). Defaults to FALSE.
)
summary(dl_model_sim1)
plot(dl_model_sim1)
```

#### Model with 12 Effects 
- Increase in Mean (Logarathmic Drift) per feature
- Increase in 3 X Covariance per feature
- Increase in 1.5 X sigma mean shift per feature
```{r}
#Simulation 1
#generate multivariate normal data
#parameters from a training sample
n<-1000 #incontrol observations 
Data<-c()
Data0<-c()
Data1<-c()
Data2<-c()
Data3<-c()
Data4<-c()
S0<-c()

#one peptide LVNELTEFAK 
mean <-c(with(data=Train,tapply(RT,INDEX=PepSeq,FUN=mean))[5],
         with(data=Train,tapply(TotalArea,INDEX=PepSeq,FUN=mean)) [5],
         with(data=Train,tapply(MassAccu,INDEX=PepSeq,FUN=mean)) [5],
         with(data=Train,tapply(FWHM,INDEX=PepSeq,FUN=mean)) [5]
)
covar<-cov(Train[Train$PepSeq=="LVNELTEFAK",c(3,6,7,8)])
#generate in-control observations

S0<-data.frame(idfile=12*n+1:(20*n),PepSeq=rep("LVNELTEFAK",n),mvrnorm(n, mean, covar))
colnames(S0)<-c("idfile","PepSeq","RT","TotalArea","MassAccu","FWHM")
#S0<- reshape(S0, idvar = "idfile", timevar = "PepSeq", direction = "wide")
RESPONSE<-c("GO")
S0 <- cbind(S0,RESPONSE)

#generate out-of-control observations
#Logarithmic drift
Data11 <-data.frame(idfile=((1):(n)),PepSeq=rep("LVNELTEFAK",n),
                  mvrnorm(n, mean+c(3.0*sqrt(covar[1,1]),1.0*sqrt(covar[2,2]),1.0*sqrt(covar[3,3]),1.0*sqrt(covar[4,4])), 
                          covar))
Data12 <-data.frame(idfile=((n+1):(2*n)),PepSeq=rep("LVNELTEFAK",n),
                  mvrnorm(n, mean+c(1.0*sqrt(covar[1,1]),3.0*sqrt(covar[2,2]),1.0*sqrt(covar[3,3]),1.0*sqrt(covar[4,4])), 
                          covar))

Data13 <-data.frame(idfile=((2*n+1):(3*n)),PepSeq=rep("LVNELTEFAK",n),
                  mvrnorm(n, mean+c(1.0*sqrt(covar[1,1]),1.0*sqrt(covar[2,2]),3.0*sqrt(covar[3,3]),1.0*sqrt(covar[4,4])), 
                          covar))

Data14 <-data.frame(idfile=((3*n + 1):(4*n)),PepSeq=rep("LVNELTEFAK",n),
                  mvrnorm(n, mean+c(1.0*sqrt(covar[1,1]),1.0*sqrt(covar[2,2]),1.0*sqrt(covar[3,3]),3.0*sqrt(covar[4,4])), 
                          covar))

#generate out-of-control observations for a 3 sigma fluctuation in all features large shift
covar21<- covar
covar21[1,1]<-3*covar[1,1]

Data21<-data.frame(idfile=((4*n+1):(5*n)),PepSeq=rep("LVNELTEFAK",n),
                  mvrnorm(n, mean, 
                          covar21))
covar22<- covar
covar22[2,2]<-3*covar[2,2]

Data22<-data.frame(idfile=((5*n+1):(6*n)),PepSeq=rep("LVNELTEFAK",n),
                  mvrnorm(n, mean, 
                          covar22))


covar23<-covar
covar23[3,3]<-3*covar[3,3]

Data23<-data.frame(idfile=((6*n+1):(7*n)),PepSeq=rep("LVNELTEFAK",n),
                  mvrnorm(n, mean, 
                          covar23))

covar24<-covar
covar24[4,4]<-3*covar[4,4]

Data24<-data.frame(idfile=((7*n+1):(8*n)),PepSeq=rep("LVNELTEFAK",n),
                  mvrnorm(n, mean, 
                          covar24))


#generate out-of-control observations for a 1.5 sigma step shift 
Data31 <-data.frame(idfile=(8*n+1):(9*n),PepSeq=rep("LVNELTEFAK",n),
                  mvrnorm(n, mean+c(1.5*sqrt(covar[1,1]),1.0*sqrt(covar[2,2]),1.0*sqrt(covar[3,3]),1.0*sqrt(covar[4,4])), 
                          covar))

Data32 <-data.frame(idfile=(9*n+1):(10*n),PepSeq=rep("LVNELTEFAK",n),
                  mvrnorm(n, mean+c(1.0*sqrt(covar[1,1]),1.5*sqrt(covar[2,2]),1.0*sqrt(covar[3,3]),1.0*sqrt(covar[4,4])), 
                          covar))

Data33 <-data.frame(idfile=(10*n+1):(11*n),PepSeq=rep("LVNELTEFAK",n),
                  mvrnorm(n, mean+c(1.0*sqrt(covar[1,1]),1.0*sqrt(covar[2,2]),1.5*sqrt(covar[3,3]),1.0*sqrt(covar[4,4])), 
                          covar))

Data34 <-data.frame(idfile=(11*n+1):(12*n),PepSeq=rep("LVNELTEFAK",n),
                  mvrnorm(n, mean+c(1.0*sqrt(covar[1,1]),1.0*sqrt(covar[2,2]),1.0*sqrt(covar[3,3]),1.5*sqrt(covar[4,4])), 
                          covar))


#Merge all four type of disturbances + in-control observations
Data0<-rbind(Data11,Data12, Data13, Data14, Data21, Data22, Data23, Data24, Data31, Data32, Data33, Data34) #Data0<-reshape(Data0, idvar = "idfile", timevar = "PepSeq", direction = "wide")
RESPONSE<-c("NOGO")
colnames(Data0) <- c("idfile","PepSeq","RT","TotalArea","MassAccu","FWHM")
Data0 <- cbind(Data0,RESPONSE)
Data0 <-rbind(S0,Data0)
```

```{r eval = False}
#select random ind for train and test 
set.seed(123)

## 75% of the sample size
smp_size <- floor(0.8 * nrow(Data0))

## set the seed to make your partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(Data0)), size = smp_size)

train_sim_all <- Data0[train_ind,]
test_sim_all <- Data0[-train_ind,]

#min max scaling & centering the train data 0-1
train_maxs <- apply(train_sim_all %>% dplyr::select(-c(idfile, PepSeq, RESPONSE)), 2, max) 
train_mins <- apply(train_sim_all %>% dplyr::select(-c(idfile, PepSeq, RESPONSE)), 2, min)
train_sim_all_scaled_data <- as.data.frame(scale(train_sim_all %>% dplyr::select(-c(idfile,PepSeq,RESPONSE)), center = train_mins, scale = train_maxs - train_mins))

#min max scaling & centering the test data 0-1
test_maxs <- apply(test_sim_all %>% dplyr::select(-c(idfile,PepSeq, RESPONSE)), 2, max) 
test_mins <- apply(test_sim_all %>% dplyr::select(-c(idfile, PepSeq, RESPONSE)), 2, min)
test_sim_all_scaled_data <- as.data.frame(scale(test_sim_all %>% dplyr::select(-c(idfile,PepSeq, RESPONSE)), center = test_mins, scale = test_maxs - test_mins))



train_sim_all_scaled_data$RESPONSE <- as.factor(train_sim_all$RESPONSE)
train_sim_all_scaled_data$idfile <- train_sim_all$idfile

test_sim_all_scaled_data$RESPONSE <- as.factor(test_sim_all$RESPONSE)
test_sim_all_scaled_data$idfile <- test_sim_all$idfile
```



```{r}
#Building Neural Network
library(h2o)
#generate same set of random numbers (for reproducibility)
set.seed(121)

#launch h2o cluster
localH2O <- h2o.init(nthreads = -1)


#import r objects to h2o cloud
train_h2o <- as.h2o(train_sim_all)
test_h2o <- as.h2o(test_sim_all)

set.seed(100)

dl_model_sim1 <- h2o.deeplearning(
  model_id="dl_model_first", 
  training_frame=train_h2o, 
  validation_frame = test_h2o,
  x= colnames(train_h2o[,3:6]),
  y= "RESPONSE",
  activatio="Tanh",  
  hidden=c(20,20), 
  standardize = TRUE, #standardizes the data
  loss= "CrossEntropy",
  stopping_metric="logloss",
  stopping_rounds = 10,
  stopping_tolerance=0.00001,
  adaptive_rate = TRUE,
  shuffle_training_data = TRUE, 
  rate = 0.005, # Defaults to 0.005 adaptive enabled so cannot specify the learning rare 
  mini_batch_size = 1,# defaults to 1 
  epochs=200,
  seed = 123, # give seed 
  export_weights_and_biases = T, # export weights and biases defaults to false
  reproducible = T # Force reproducibility on small data (will be slow - only uses 1 thread). Defaults to FALSE.
)
summary(dl_model_sim1)
plot(dl_model_sim1)
```




